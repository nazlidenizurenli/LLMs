{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncPage[Model](data=[Model(id='omni-moderation-2024-09-26', created=1732734466, object='model', owned_by='system'), Model(id='gpt-4o-mini-audio-preview-2024-12-17', created=1734115920, object='model', owned_by='system'), Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview-2024-10-01', created=1727389042, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview', created=1727460443, object='model', owned_by='system'), Model(id='gpt-4o-mini-realtime-preview-2024-12-17', created=1734112601, object='model', owned_by='system'), Model(id='gpt-4o-mini-realtime-preview', created=1734387380, object='model', owned_by='system'), Model(id='o1-mini-2024-09-12', created=1725648979, object='model', owned_by='system'), Model(id='o1-preview-2024-09-12', created=1725648865, object='model', owned_by='system'), Model(id='o1-mini', created=1725649008, object='model', owned_by='system'), Model(id='o1-preview', created=1725648897, object='model', owned_by='system'), Model(id='gpt-4o-mini-audio-preview', created=1734387424, object='model', owned_by='system'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2024-10-01', created=1727131766, object='model', owned_by='system'), Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'), Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview-2024-12-17', created=1734034239, object='model', owned_by='system'), Model(id='gpt-4o-2024-11-20', created=1739331543, object='model', owned_by='system'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system'), Model(id='gpt-4o', created=1715367049, object='model', owned_by='system'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2024-12-17', created=1733945430, object='model', owned_by='system'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4o-realtime-preview', created=1727659998, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system'), Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system'), Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system'), Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'), Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system'), Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'), Model(id='omni-moderation-latest', created=1731689265, object='model', owned_by='system')], object='list')\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os, sys\n",
    "import textwrap\n",
    "\n",
    "# Environment Setup and Connection to OpenAI\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "  raise ValueError(\"OpenAI API key not found. Please set OPENAI_API_KEY in your environment variables.\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(client.models.list())\n",
    "\n",
    "# Select GPT model\n",
    "gptmodel = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the connection is successful. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Test connection to OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    model=gptmodel,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Test connection successful?\"}],\n",
    ")\n",
    "print(response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_with_full_text(input_text):\n",
    "    text_input = '\\n'.join(input_text)\n",
    "    prompt = f\"Please elaborate on the following subject: {text_input}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=gptmodel,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert on Natural Language Processing.\"},\n",
    "                {\"role\": \"system\", \"content\": \"You can explain read the input and answer in detail.\"},\n",
    "                {\"role\": \"system\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_completion_tokens=200\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_formatted_response(response):\n",
    "    wrapper = textwrap.TextWrapper(width=80)\n",
    "    wrapped_text = wrapper.fill(text=response)\n",
    "    print(\"Response: \")\n",
    "    print(\"-----------------\") \n",
    "    print(wrapped_text)\n",
    "    print(\"-----------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_records = [\n",
    "    # âœ… Best definition (should be picked)\n",
    "    \"Retrieval-Augmented Generation (RAG) is an advanced framework that enhances Large Language Models (LLMs) by integrating a retrieval mechanism. Instead of relying solely on pre-trained knowledge, a RAG system retrieves relevant information from an external knowledge base (e.g., a document store, database, or vector index) and uses it to generate more accurate and contextually relevant responses.\",\n",
    "\n",
    "    # ðŸ”¸ Somewhat related, but vague\n",
    "    \"RAG is a technique used in AI to improve performance. It involves retrieval of external information, which is then used in a response generation process.\",\n",
    "\n",
    "    # âŒ Completely vague (not useful)\n",
    "    \"RAG is a system that retrieves documents from databases and displays them to users. It is commonly used in search engines.\",\n",
    "\n",
    "    # âŒ Totally unrelated\n",
    "    \"Rags are often used in cleaning, polishing, and household tasks. They come in various materials like cotton and microfiber.\",\n",
    "\n",
    "    # âŒ Off-topic but looks similar\n",
    "    \"AI-powered search engines retrieve relevant documents based on queries. They use ranking algorithms and keyword matching to provide accurate results.\",\n",
    "\n",
    "    # âŒ Another vague response\n",
    "    \"Retrieval systems are important for fetching stored information, whether itâ€™s in AI models or traditional database queries.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) is an advanced framework that enhances\n",
      "Large Language Models (LLMs) by integrating a retrieval mechanism. Instead of\n",
      "relying solely on pre-trained knowledge, a RAG system retrieves relevant\n",
      "information from an external knowledge base (e.g., a document store, database,\n",
      "or vector index) and uses it to generate more accurate and contextually relevant\n",
      "responses. RAG is a technique used in AI to improve performance. It involves\n",
      "retrieval of external information, which is then used in a response generation\n",
      "process. RAG is a system that retrieves documents from databases and displays\n",
      "them to users. It is commonly used in search engines. Rags are often used in\n",
      "cleaning, polishing, and household tasks. They come in various materials like\n",
      "cotton and microfiber. AI-powered search engines retrieve relevant documents\n",
      "based on queries. They use ranking algorithms and keyword matching to provide\n",
      "accurate results. Retrieval systems are important for fetching stored\n",
      "information, whether itâ€™s in AI models or traditional database queries.\n"
     ]
    }
   ],
   "source": [
    "paragraph = ' '.join(db_records)\n",
    "wrapped_text = textwrap.fill(paragraph, width=80)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "-----------------\n",
      "A rag store is a retail or wholesale shop that sells various types of used or\n",
      "second-hand clothes, also known as \"rags\". These stores are often found in\n",
      "lower-income neighborhoods where people are looking for affordable clothing\n",
      "options. Some rag stores also sell other used items like furniture, household\n",
      "goods, and toys.   In a broader context, the term \"rag store\" can also refer to\n",
      "a business that buys and sells scrap materials, including textiles. These\n",
      "businesses play a crucial role in the recycling industry, as they help to reduce\n",
      "waste and promote the reuse of materials.   It's important to note that the term\n",
      "\"rag store\" is not commonly used in all regions or countries. In some places,\n",
      "these types of stores might be referred to as thrift stores, second-hand stores,\n",
      "or charity shops.\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Define a rag store\"\n",
    "llm_response = call_llm_with_full_text(query)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(text1, text2):\n",
    "    \" Convert text pieces into TfidfVectors and compare using cosine similarity.\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        use_idf=True,\n",
    "        norm='l2',\n",
    "        # ngram_range=(1, 2),\n",
    "        sublinear_tf=True,\n",
    "        analyzer='word'\n",
    "    )\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.5803329846765686\n",
      "Similarity Score: 0.716811741443062\n"
     ]
    }
   ],
   "source": [
    "text1 = \"FPGA constraints define timing and placement.\"\n",
    "text2 = \"Constraints in FPGA set placement and timing rules.\"\n",
    "\n",
    "text3 = \"FPGA constraints define timing and placement rules.\"\n",
    "text4 = \"Timing and placement rules are defined by FPGA constraints.\"\n",
    "\n",
    "score = calculate_cosine_similarity(text1, text2)\n",
    "print(\"Similarity Score:\", score)\n",
    "\n",
    "score2 = calculate_cosine_similarity(text3, text4)\n",
    "print(\"Similarity Score:\", score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'define', 'rag', 'store'}\n",
      "0.1528049645732037\n",
      "0.08434796492247011\n",
      "0.10371551133313005\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Best Keyword Score:  2\n",
      "Best Matching Record:  Retrieval-Augmented Generation (RAG) is an advanced framework that enhances Large Language Models (LLMs) by integrating a retrieval mechanism. Instead of relying solely on pre-trained knowledge, a RAG system retrieves relevant information from an external knowledge base (e.g., a document store, database, or vector index) and uses it to generate more accurate and contextually relevant responses.\n",
      "Response: \n",
      "-----------------\n",
      "Retrieval-Augmented Generation (RAG) is an advanced framework that enhances\n",
      "Large Language Models (LLMs) by integrating a retrieval mechanism. Instead of\n",
      "relying solely on pre-trained knowledge, a RAG system retrieves relevant\n",
      "information from an external knowledge base (e.g., a document store, database,\n",
      "or vector index) and uses it to generate more accurate and contextually relevant\n",
      "responses.\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_best_match_keyword_similarity(query, db_records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "\n",
    "    query_keywords = set(query.lower().split())\n",
    "    print(query_keywords)\n",
    "\n",
    "    for record in db_records:\n",
    "        record_keywords = set(record.lower().split())\n",
    "        common_keywords = query_keywords.intersection(record_keywords)\n",
    "        current_score = len(common_keywords)\n",
    "        similarity = calculate_cosine_similarity(query, record)\n",
    "        print(similarity)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "    \n",
    "    return best_score, best_record\n",
    "\n",
    "best_keyword_score, best_matching_record = find_best_match_keyword_similarity(query, db_records)\n",
    "print(f\"Best Keyword Score: \", best_keyword_score)\n",
    "print(f\"Best Matching Record: \", best_matching_record)\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "-----------------\n",
      "Retrieval-Augmented Generation (RAG) is a sophisticated framework that enhances\n",
      "Large Language Models (LLMs) by incorporating a retrieval mechanism. This\n",
      "mechanism is a significant departure from traditional LLMs, which rely solely on\n",
      "pre-trained knowledge.  In a RAG system, instead of generating responses based\n",
      "solely on pre-existing knowledge, the model retrieves relevant information from\n",
      "an external knowledge base. This knowledge base could be a document store, a\n",
      "database, or a vector index. The retrieved information is then used to generate\n",
      "responses.  The primary advantage of this approach is that it allows the model\n",
      "to generate more accurate and contextually relevant responses. This is because\n",
      "the model has access to a broader range of information than what is available in\n",
      "its pre-trained knowledge.   For example, if a user asks a question about a\n",
      "recent event, a traditional LLM might struggle to provide an accurate answer if\n",
      "it was trained on data that predates the event. However, a RAG system could\n",
      "retrieve information about the event from\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_input = query + \":\" + best_matching_record\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
