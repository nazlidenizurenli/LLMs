{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_k_GEcxfdJm_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D3o1mVG3dSxH"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "openai.api_key = api_key\n",
        "if not api_key:\n",
        "  raise ValueError(\"Missing OpenAi Key setting.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEcG0sXidZx0",
        "outputId": "b49be5a4-8520-47cb-8f6b-8d2b618cf9e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available models:\n",
            "dall-e-3\n",
            "dall-e-2\n",
            "o1-mini-2024-09-12\n",
            "gpt-4o-mini-realtime-preview-2024-12-17\n",
            "o1-preview-2024-09-12\n",
            "o1-mini\n",
            "o1-preview\n",
            "gpt-4o-mini-realtime-preview\n",
            "gpt-4o-mini-audio-preview-2024-12-17\n",
            "whisper-1\n",
            "gpt-4-turbo\n",
            "gpt-4o-mini-audio-preview\n",
            "babbage-002\n",
            "omni-moderation-latest\n",
            "omni-moderation-2024-09-26\n",
            "tts-1-hd-1106\n",
            "gpt-4o-2024-08-06\n",
            "gpt-4o-audio-preview-2024-12-17\n",
            "gpt-4o\n",
            "gpt-4-turbo-preview\n",
            "gpt-4o-audio-preview\n",
            "gpt-4-0125-preview\n",
            "chatgpt-4o-latest\n",
            "gpt-4o-2024-05-13\n",
            "tts-1-hd\n",
            "gpt-4-turbo-2024-04-09\n",
            "gpt-4o-mini\n",
            "gpt-4o-mini-2024-07-18\n",
            "tts-1\n",
            "tts-1-1106\n",
            "davinci-002\n",
            "gpt-3.5-turbo-1106\n",
            "gpt-4o-2024-11-20\n",
            "gpt-3.5-turbo-instruct\n",
            "gpt-3.5-turbo-instruct-0914\n",
            "gpt-3.5-turbo-0125\n",
            "gpt-4o-realtime-preview-2024-12-17\n",
            "gpt-3.5-turbo\n",
            "gpt-4o-realtime-preview\n",
            "gpt-3.5-turbo-16k\n",
            "text-embedding-3-small\n",
            "gpt-4\n",
            "text-embedding-ada-002\n",
            "gpt-4-1106-preview\n",
            "gpt-4o-audio-preview-2024-10-01\n",
            "gpt-4-0613\n",
            "text-embedding-3-large\n",
            "gpt-4o-realtime-preview-2024-10-01\n"
          ]
        }
      ],
      "source": [
        "mymodel = 'gpt-4-turbo'\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "models = client.models.list()\n",
        "print(\"Available models:\")\n",
        "for model in models.data:\n",
        "    print(f\"{model.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R0QIweDPg44_"
      },
      "outputs": [],
      "source": [
        "def get_deterministic_llm_answer(user_prompt, model='gpt-3.5-turbo'):\n",
        "  messages = [\n",
        "      {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "      {'role': 'user', 'content': user_prompt}\n",
        "  ]\n",
        "  response = client.chat.completions.create(\n",
        "      model=model,\n",
        "      messages=messages,\n",
        "      temperature=0,\n",
        "      max_tokens=100\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OEE2E7apiBF9"
      },
      "outputs": [],
      "source": [
        "def get_creative_llm_answer(user_prompt, model='gpt-3.5-turbo'):\n",
        "  messages = [\n",
        "      {'role': 'system', 'content': 'You are a science expert assistant.'},\n",
        "      {'role': 'user', 'content': user_prompt}\n",
        "  ]\n",
        "  response = client.chat.completions.create(\n",
        "      model=model,\n",
        "      messages=messages,\n",
        "      temperature=1.2,\n",
        "      max_tokens=100\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c7cbl_YgiC1w"
      },
      "outputs": [],
      "source": [
        "prompt1 = \"I would like to learn about the nature. Can you explain it in a sentence.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FvP5VH2XiDGY"
      },
      "outputs": [],
      "source": [
        "prompt2 = \"I would like to know what AI is. Can you explain in 2-3 sentences.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cIZdWZPuoZ4_"
      },
      "outputs": [],
      "source": [
        "prompt3 = \"Give me a title for a poem on nature\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ox6yRlWQo973"
      },
      "outputs": [],
      "source": [
        "prompt4 = \"Describe a futuristic city in 2145\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "FmY7UOUYiPEv",
        "outputId": "8371e948-9198-4922-8cfe-dbf04891635d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In the year 2145, the cityscape has transformed dramatically, reflecting advancements in technology, sustainability, and human lifestyle. The city, known as Neo-Terra, is a model of innovation and ecological harmony, designed to accommodate a growing population in a world where climate change has necessitated significant adaptations.\\n\\n### Architecture and Infrastructure\\n\\nNeo-Terra is characterized by its vertical architecture, with skyscrapers reaching unprecedented heights thanks to advances in materials science, including carbon nanotubes and self-healing concrete'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's see what the deterministic LLM says\n",
        "get_deterministic_llm_answer(prompt4, mymodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "18e6sTkgiUk2",
        "outputId": "3696f16a-8185-44d9-c8c0-66552433ef85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The futuristic city of 2145 represents a harmonious blend of advanced technology, sustainable design, and vibrant multicultural society. Urban centers in 2145 are meticulously designed to minimize environmental impact, maximize human well-being, and foster economic vitality. Here's a detailed look into various components:\\n\\n### Infrastructure\\n1. **Ecological Buildings:**\\n   - Skyscrapers and residential blocks constructed Forgoing harmful materials and built using ultra-light, self-cleaning, and air-purifying materials that adapt\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's see what the creative LLM says\n",
        "get_creative_llm_answer(prompt4, mymodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WxSgGNQzzebY"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7Anrmrgz5yQ",
        "outputId": "4a0fb3c2-349f-4657-af1d-f0634981dc6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_15015/1517976950.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  chat = ChatOpenAI(model_name=mymodel, api_key=api_key, temperature=0)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f69e6366f90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f69e63d8a50>, model_name='gpt-4-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_proxy='')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat = ChatOpenAI(model_name=mymodel, api_key=api_key, temperature=0)\n",
        "chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7o56Td2d24gR"
      },
      "outputs": [],
      "source": [
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB6fXehd3Jkq",
        "outputId": "000eff8d-d362-44f3-8d13-ab6fc013c397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['style', 'text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'), additional_kwargs={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "print(prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-C-AI7e3QiZ",
        "outputId": "72b8a652-c579-4219-9e3d-23a757be54e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['style', 'text']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt\n",
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tjXoZxE430DV"
      },
      "outputs": [],
      "source": [
        "customer_style = \"\"\" American English in a calm respectable tone. \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MBDuTRk3D2yi"
      },
      "outputs": [],
      "source": [
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse, \\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WF3lGH6yD63p"
      },
      "outputs": [],
      "source": [
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ieQcCIID_Hh",
        "outputId": "3e64bbad-db5d-4046-cb4b-df5a54503192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"Translate the text that is delimited by triple backticks into a style that is  American English in a calm respectable tone. . text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "print(customer_messages[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hoK-cGXLEB56"
      },
      "outputs": [],
      "source": [
        "customer_response = chat.invoke(customer_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnEyTKepEMVO",
        "outputId": "846611af-7501-4737-df5d-8d1b0878f7c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "I'm really frustrated that my blender lid came off and splattered my kitchen walls with smoothie! To make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I need your help right now!\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(customer_response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRA5vYnNEXo3",
        "outputId": "c0a9b916-78e0-4187-b11f-d168b70b3a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Subject: Inquiry Regarding an Appointment\\n\\nDear Mrs. Müller,\\n\\nI hope this message finds you well. May I kindly request the possibility of scheduling an appointment for the upcoming week? Could you please inform me of your available times? Thank you very much in advance.\\n\\nYours sincerely,\\nMax Mustermann' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 104, 'total_tokens': 164, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-turbo', 'system_fingerprint': 'fp_f17929ee92', 'finish_reason': 'stop', 'logprobs': None} id='run-e2753140-44ca-4d4f-b4c7-fe006806b0db-0'\n"
          ]
        }
      ],
      "source": [
        "# All steps of using LangChain for LLM deployment\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "# Step 1: Define Langchain Client\n",
        "\"\"\"\"\"\"\n",
        "langchain_client = ChatOpenAI(model_name=mymodel, api_key=api_key, temperature=0)\n",
        "\n",
        "# Step 2: Build template: This is what we are sending to the LLM, indicating the inputs from user using {}.\n",
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "# Step 3: Define input string\n",
        "customer_email = \"\"\"\n",
        "Betreff: Anfrage zu einem Termin \\\n",
        "\n",
        "Sehr geehrte Frau Müller, \\\n",
        "\n",
        "ich hoffe, es geht Ihnen gut. Könnten wir einen Termin für nächste Woche vereinbaren? Bitte teilen Sie mir Ihre Verfügbarkeit mit. Vielen Dank im Voraus. \\\n",
        "\n",
        "Mit freundlichen Grüßen \\\n",
        "Max Mustermann\"\"\"\n",
        "\n",
        "# Step 3: Define translation style\n",
        "customer_style = \"\"\"Polite formal English\"\"\"\n",
        "\n",
        "# Step 4: Create prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "\n",
        "# Step 5: Build customer message by joining input and template\n",
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)\n",
        "\n",
        "# Step 6: Send message to LLM\n",
        "translated_response = langchain_client.invoke(customer_messages)\n",
        "\n",
        "# Check result\n",
        "print(translated_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "llm_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
